\chapter{Experimental Evaluation}

%\section{Methodology}

The focus of this chapter is:
\begin{enumerate}
\item the setup of the benchmark system,
\item the definition of the benchmarks, and
\item the results of the benchmarks
\end{enumerate}

%
%The memory usage is examined using the ``Massif'' tool which is part of the \texttt{valgrind} suite.\cite{valgrind}

\section{Benchmark System}
  \subsection{Hardware Setup}

The processor of the benchmark system is an Intel\textregistered~ Core\texttrademark~ i7-4771 CPU clocked at 3.50 GHz with 4 cores and hyperthreading enabled (which means 8 threads are available).
The benchmark system has a total amount of 16 GB of DDR3 RAM clocked at 1333 MHz available.

The root partition of the operating system is located on a ``Samsung SSD 840'' solid state drive (SSD).
It is installed on an ext4 filesystem.

The \texttt{hdparm} program gives an idea of how much throughput the SSD can handle. The results are listed in the table \ref{eval-ssd} on page \pageref{eval-ssd}.

\begin{table}[h]
\centering
\caption{Read performance of the benchmark SSD}
\label{eval-ssd}
\begin{tabular}{lrrr}
\textbf{}                   & \textbf{data read} & \textbf{time} & \textbf{result} \\ \hline
Timing cached reads         & $28536$ MB         & $1.99$ s      & $14316.64$ MB/s \\
Timing buffered disk reads: & $1608$ MB          & $3.00$ s      & $535.47$ MB/s   \\ \hline
\end{tabular}
\end{table}

All benchmarks are performend on the SSD.

  \subsection{Software Setup}

The operating system Fedora 27 is used for executing the benchmarks.
Elektra version 0.8.21 at the git commit \texttt{dfa9bb8}\footnote{The full commit hash is: dfa9bb89ada39996cac5c1abd21481e1e2181ad9} is installed on the system.
\todo{update Elektra version and commit}

The most important program versions are listed below:

\begin{itemize}
  \item clang version 5.0.1 (tags/RELEASE\_501/final)
  \item Botan version 1.10.17-1.fc27
  \item OpenSSL version 1.1.0g-1.fc27
  \item libgcrypt version 1.8.2-1.fc27
  \item GnuPG version 2.2.4-1.fc27
\end{itemize}

Listing \ref{eval-compile} on page \pageref{eval-compile} shows how \elektra ~ is compiled on the benchmark system.

\begin{code}[label=eval-compile,language=bash,caption={Elektra compile options for the benchmarks}]
mkdir build && cd build
cmake -GNinja \
    -DBUILD_STATIC=OFF \
    -DCMAKE_C_COMPILER=clang \
    -DCMAKE_CXX_COMPILER=clang++ \
    -DBUILD_DOCUMENTATION=OFF \
    -DCMAKE_INSTALL_PREFIX=/usr \
    ..
ninja install
\end{code}

  \subsection{Time Measurement}

The runtime of a benchmark is measured using the system time, which is returned by the system function \texttt{gettimeofday ()}.
The time measurement is abstracted in a class called \texttt{Timer}.
Listing \ref{eval-time} on page \pageref{eval-time} demonstrates how a benchmark is written.

\begin{code}[label=eval-time,language=C,caption={Time measurement for the benchmarks}]
void do_benchmark ()
{
  Timer t();

  // begin of measurement
  t.start ();

  action_to_be_measured ();

  // end of measurement
  t.end ();
}
\end{code}

\todo{describe statistical method (median, number of runs, iterations/run etc.)}

\section{Benchmark 1 -- Runtime Comparison}

This benchmark examines the runtime performance of the \crypto ~.
The benchmark compares the duration of the \texttt{kdb set} and \texttt{kdb get} phases:

\begin{enumerate}
\item without the \crypto ,
\item with the \texttt{crypto\_openssl} plugin variant,
\item the \texttt{crypto\_gcrypt} plugin variant, and 
\item the \texttt{crypto\_botan} plugin variant.
\end{enumerate}

For each benchmark variant an Elektra backend is set up.
For every backend a configuration setting with 100 keys \todo{verify number, possibly increase} is generated.
First the duration of the \texttt{kdb set} method is being measured.
Then the duration of the \texttt{kdb get} method is being measured.

  \subsection{Code Example}

The source code of the benchmark is distributed with Elektra, and it is located at\\
\texttt{libs/tools/benchmarks/benchmark\_crypto\_comparison.cpp}.

Listing \ref{eval-1-code} on page \pageref{eval-1-code} is an excerpt of the benchmark code.
The listing gives an idea about how the measurement is accomplished.
The listing has been slightly modified to improve readability, but the execution flow has not been altered.

\begin{code}[label=eval-1-code,language=C,caption={Excerpt of Benchmark 1}]
static Timer t (plugin_variant_names[VARIANT]);
Key mp = mountBackend<VARIANT> (iteration);
{
  KDB kdb;
  KeySet ks;

  kdb.get (ks, mp);
  for (int i = 0; i < 100; ++i)
  {
    ks.append (Key (mp.getName () + "/k" + std::to_string (i),
      KEY_VALUE, "value",
      KEY_META, "crypto/encrypt", "1",
      KEY_END));
  }

  t.start (); // start of the measurement
  kdb.set (ks, mp);
  t.stop (); // end of the measurement

  kdb.close ();
}
std::cout << t;
\end{code}

  \subsection{Results of the Runtime Comparison}


\begin{figure}[h]
\center
\caption{Boxplots of the \texttt{kdb get} runtime with $n = 500$ configuration settings}
\label{eval-boxplot-get}
\includegraphics[width=\columnwidth]{plots/boxplot_500_get.pdf}
\end{figure}

\begin{figure}[h]
\center
\caption{Boxplots of the \texttt{kdb set} runtime with $n = 500$ configuration settings}
\label{eval-boxplot-set}
\includegraphics[width=\columnwidth]{plots/boxplot_500_set.pdf}
\end{figure}


\begin{figure}[h]
\center
\caption{Runtime comparison of \texttt{kdb get}}
\label{eval-runtime-comp-get}
\includegraphics[width=\columnwidth]{plots/comp_median_get.pdf}
\end{figure}

\begin{figure}[h]
\center
\caption{Runtime comparison of \texttt{kdb set}}
\label{eval-runtime-comp-set}
\includegraphics[width=\columnwidth]{plots/comp_median_set.pdf}
\end{figure}





\section{Interpretation Of The Results}



